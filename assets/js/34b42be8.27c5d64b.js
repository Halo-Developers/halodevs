"use strict";(self.webpackChunkcode_esi_docs=self.webpackChunkcode_esi_docs||[]).push([[2305],{3905:function(e,t,n){n.d(t,{Zo:function(){return s},kt:function(){return c}});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var p=a.createContext({}),m=function(e){var t=a.useContext(p),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},s=function(e){var t=m(e.components);return a.createElement(p.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,p=e.parentName,s=o(e,["components","mdxType","originalType","parentName"]),d=m(n),c=r,k=d["".concat(p,".").concat(c)]||d[c]||u[c]||i;return n?a.createElement(k,l(l({ref:t},s),{},{components:n})):a.createElement(k,l({ref:t},s))}));function c(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,l=new Array(i);l[0]=d;var o={};for(var p in t)hasOwnProperty.call(t,p)&&(o[p]=t[p]);o.originalType=e,o.mdxType="string"==typeof e?e:r,l[1]=o;for(var m=2;m<i;m++)l[m]=n[m];return a.createElement.apply(null,l)}return a.createElement.apply(null,n)}d.displayName="MDXCreateElement"},820:function(e,t,n){n.r(t),n.d(t,{assets:function(){return s},contentTitle:function(){return p},default:function(){return c},frontMatter:function(){return o},metadata:function(){return m},toc:function(){return u}});var a=n(7462),r=n(3366),i=(n(7294),n(3905)),l=["components"],o={title:"Linear Regression with multiple variables",sidebar_position:4},p="Problem",m={unversionedId:"ml/multiple_features",id:"ml/multiple_features",title:"Linear Regression with multiple variables",description:"unsted of 2 features (plan), we have more than 2 and we wanna have a formula that represents the relation between those features",source:"@site/docs/ml/multiple_features.md",sourceDirName:"ml",slug:"/ml/multiple_features",permalink:"/halodevs/docs/ml/multiple_features",editUrl:"https://github.com/halodevelopers/halodevs/tree/main/docs/ml/multiple_features.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{title:"Linear Regression with multiple variables",sidebar_position:4},sidebar:"tutorialSidebar",previous:{title:"Unsupervised Learning",permalink:"/halodevs/docs/ml/unsupervised"}},s={},u=[{value:"Example",id:"example",level:2},{value:"General",id:"general",level:2},{value:"Gradient descent",id:"gradient-descent",level:2},{value:"Definition",id:"definition",level:3},{value:"How to use",id:"how-to-use",level:3},{value:"Speed",id:"speed",level:3}],d={toc:u};function c(e){var t=e.components,n=(0,r.Z)(e,l);return(0,i.kt)("wrapper",(0,a.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"problem"},"Problem"),(0,i.kt)("p",null,"unsted of 2 features (plan), we have more than 2 and we wanna have a formula that represents the relation between those features"),(0,i.kt)("h1",{id:"how-to-solve-with-hypothesis"},"How to solve with Hypothesis?"),(0,i.kt)("h2",{id:"example"},"Example"),(0,i.kt)("p",null,"Let's say we have 4 features ",(0,i.kt)("em",{parentName:"p"},"(variables)")," and 1 label ",(0,i.kt)("em",{parentName:"p"},"(result)")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"v1, v2, v3, v4"),(0,i.kt)("li",{parentName:"ul"},"l1")),(0,i.kt)("p",null,"Let's put:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"n"),": number of features ",(0,i.kt)("em",{parentName:"li"},"(4)")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"x(i)"),": is the ",(0,i.kt)("inlineCode",{parentName:"li"},"vector")," of the n features at the ",(0,i.kt)("strong",{parentName:"li"},"(i ",(0,i.kt)("em",{parentName:"strong"},"th"),")")," row, exp: x(2) = ","[15,154,25,38]"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"x(i)","[j]"),": the ",(0,i.kt)("inlineCode",{parentName:"li"},"value")," of feature number j in the vector x(i)")),(0,i.kt)("p",null,"Our Hypothesis form:\n",(0,i.kt)("inlineCode",{parentName:"p"},"h(x) = O0 + O1\\*x1 + O2\\*x2 + O3\\*x3 + O4\\*x4")),(0,i.kt)("h2",{id:"general"},"General"),(0,i.kt)("p",null,"For a Hypothesis form with ",(0,i.kt)("strong",{parentName:"p"},"n")," features:\n",(0,i.kt)("inlineCode",{parentName:"p"},"h(x) = O0 + O1\\*x1 + O2\\*x2 + O3\\*x3 + ... + On\\*xn")),(0,i.kt)("p",null,"Let's add x0 = 1 to our Hypothesis:\n",(0,i.kt)("inlineCode",{parentName:"p"},"h(x) = O0\\*x0 + O1\\*x1 + O2\\*x2 + O3\\*x3 + ... + On\\*xn")),(0,i.kt)("p",null,"Let's simplify it:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"- O = [O0\n       O1\n       O2\n       O3\n       ..\n       On]\n- X = [x0\n       x1\n       x2\n       x3\n       ..\n       xn]\n- O(T) = [O0 O1 O2 O3 .. On]\n")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"O: ",(0,i.kt)("em",{parentName:"li"},"(n+1)x1")," matrix"),(0,i.kt)("li",{parentName:"ul"},"X: ",(0,i.kt)("em",{parentName:"li"},"(n+1)x1")," matrix"),(0,i.kt)("li",{parentName:"ul"},"O(T): ",(0,i.kt)("em",{parentName:"li"},"1x(n+1)")," matrix")),(0,i.kt)("p",null,"We have:"),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"h(x) = O(T).X")),(0,i.kt)("h1",{id:"fit-the-parameters-of-the-hypothesis-with-gradient-descent-for-multiple-variables"},"Fit the parameters of the Hypothesis with Gradient descent for multiple variables"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Our parameters ",(0,i.kt)("inlineCode",{parentName:"li"},"O vector")),(0,i.kt)("li",{parentName:"ul"},"Cost function: ",(0,i.kt)("inlineCode",{parentName:"li"},"J(O) = (1/2m).Sigma_i_1_m(h(x(i)) - y(i))\xb2")),(0,i.kt)("li",{parentName:"ul"},"Gradient descent:")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"Repeat until convergence: {\n    Oj := Oj - [alpha].D(J(O))/D(Oj) = Oj - [alpha].(1/m).Sigma_i_1_m(h(x(i)) - y(i))(xj(i))\n    (simuntalneously update for every j = 0,...,n)\n}\n")),(0,i.kt)("h2",{id:"gradient-descent"},"Gradient descent"),(0,i.kt)("h3",{id:"definition"},"Definition"),(0,i.kt)("p",null,"It is an optimization technique that can ",(0,i.kt)("strong",{parentName:"p"},"find the minimum")," of an ",(0,i.kt)("strong",{parentName:"p"},"objective function")," by taking a step in the direction of the maximum rate of ",(0,i.kt)("strong",{parentName:"p"},"decrease")," of the function."),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/72823374/156913246-cabe20b0-f7f1-4608-92b7-a73dce584dd5.png",alt:"image"})),(0,i.kt)("h3",{id:"how-to-use"},"How to use"),(0,i.kt)("p",null,"Consider a multi-variable function:\n",(0,i.kt)("inlineCode",{parentName:"p"},"h(O)"),", where ",(0,i.kt)("inlineCode",{parentName:"p"},"O = [O0,O2,...,On]T")),(0,i.kt)("p",null,"To find Omin where ",(0,i.kt)("inlineCode",{parentName:"p"},"h(Omin) = min"),", gradient descent uses the following steps:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Choose an initial value for ",(0,i.kt)("inlineCode",{parentName:"li"},"Omin")," randomly"),(0,i.kt)("li",{parentName:"ol"},"Choose the number of maximum iterations ",(0,i.kt)("inlineCode",{parentName:"li"},"T")),(0,i.kt)("li",{parentName:"ol"},"Choose a learning rate ",(0,i.kt)("inlineCode",{parentName:"li"},"alpha"),", between 0 and 1"),(0,i.kt)("li",{parentName:"ol"},"Repeat ",(0,i.kt)("inlineCode",{parentName:"li"},"T")," times, until h ",(0,i.kt)("strong",{parentName:"li"},"does not change")," or iterations ",(0,i.kt)("strong",{parentName:"li"},"exceeds")," ",(0,i.kt)("inlineCode",{parentName:"li"},"T"),":",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Calculate ",(0,i.kt)("inlineCode",{parentName:"li"},"\u2207(Omin) = -alpha.\u2207(h(Omin))")),(0,i.kt)("li",{parentName:"ul"},"Update ",(0,i.kt)("inlineCode",{parentName:"li"},"Omin")," by adding ",(0,i.kt)("inlineCode",{parentName:"li"},"\u2207(Omin)")," to ",(0,i.kt)("inlineCode",{parentName:"li"},"Omin"))))),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"\u2207(h(Omin)) = [D(Omin)/D(O0), D(Omin)/D(O1), ..., D(Omin)/D(On)]T")),(0,i.kt)("p",null,"Example:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py",metastring:"file.py","file.py":!0},"import matplotlib.pyplot as plt\n")),(0,i.kt)("h3",{id:"speed"},"Speed"),(0,i.kt)("p",null,"We can speed up gradient descent by having each of our input values in ",(0,i.kt)("inlineCode",{parentName:"p"},"roughly")," the ",(0,i.kt)("strong",{parentName:"p"},"same range"),"."),(0,i.kt)("p",null,"Example:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python",metastring:"example.py","example.py":!0},'import seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = [1000,1356,2105,5054,3584,5941,4152]\ny = [1.2,3.6,5.2,0.4,1.5,1.1,3.5]\n\ndf = pd.DataFrame(data={"x":x,"y":y})\n\nsns.kdeplot(data=df,x="x",y="y")\nplt.show()\n')),(0,i.kt)("p",null,"Result:\n",(0,i.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/72823374/156912459-05b14332-3a03-4d25-801a-b1acc9588d2d.png",alt:"image"})),(0,i.kt)("h1",{id:"feature-scalling"},"Feature Scalling"),(0,i.kt)("p",null,"To run Gradient descent much more ",(0,i.kt)("strong",{parentName:"p"},"faster"),", make sure features are on a ",(0,i.kt)("inlineCode",{parentName:"p"},"similar scale")),(0,i.kt)("p",null,"Divide each value by the ",(0,i.kt)("inlineCode",{parentName:"p"},"max")," value ",(0,i.kt)("inlineCode",{parentName:"p"},"x1 = x1/max"),"\n, so we will get features between ",(0,i.kt)("em",{parentName:"p"},"(-1,1)"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"[-1 <= x1 <= 1]")),(0,i.kt)("h1",{id:"mean-normalization"},"Mean Normalization"),(0,i.kt)("p",null,"Replace ",(0,i.kt)("strong",{parentName:"p"},"x(i)")," with ",(0,i.kt)("strong",{parentName:"p"},"x(i) - \xb5(i)")," to make features have approximatly ",(0,i.kt)("strong",{parentName:"p"},"0")," mean (Do not apply to x0 = 1)"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"\xb5(i)")," avrage of ",(0,i.kt)("em",{parentName:"li"},"x(i)")," in the set.")),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"x1 = (x1 - \xb51)/max")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"")))}c.isMDXComponent=!0}}]);